{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b106493d29c740298ee9bbd4ba636dcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f499c7cdd9bd48e29f40727bf1e4eab5",
              "IPY_MODEL_304aa1944f1e41f5a091f3d170504e5c",
              "IPY_MODEL_5fef9c01f74f463ca34d56ef580ae159"
            ],
            "layout": "IPY_MODEL_4ac0c2b939de420cb778b815de33c7a2"
          }
        },
        "f499c7cdd9bd48e29f40727bf1e4eab5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25c3b8cf32974fc68cb03db68bfb2fe1",
            "placeholder": "​",
            "style": "IPY_MODEL_fe8638606bd842e3ad2753402552a398",
            "value": "config.json: 100%"
          }
        },
        "304aa1944f1e41f5a091f3d170504e5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b22799bd9314bec97a327e2397d73bb",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d3d69a1ef384905b5b9dc9b991165e1",
            "value": 570
          }
        },
        "5fef9c01f74f463ca34d56ef580ae159": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fae993459e354a37af0dfa7226ed2b46",
            "placeholder": "​",
            "style": "IPY_MODEL_d741e362e04b48c0945d75da046938ca",
            "value": " 570/570 [00:00&lt;00:00, 7.15kB/s]"
          }
        },
        "4ac0c2b939de420cb778b815de33c7a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25c3b8cf32974fc68cb03db68bfb2fe1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe8638606bd842e3ad2753402552a398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b22799bd9314bec97a327e2397d73bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d3d69a1ef384905b5b9dc9b991165e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fae993459e354a37af0dfa7226ed2b46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d741e362e04b48c0945d75da046938ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2b1ad9b2be94829a2d97fe60281e7a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c374b2ba6ed4cf9bb7464390e1f1430",
              "IPY_MODEL_dec9e68150a247d1b6a8e256c133dd38",
              "IPY_MODEL_36540d305287438fb942310f668dd5cf"
            ],
            "layout": "IPY_MODEL_809d0357c5b84268a49544b477b9b379"
          }
        },
        "0c374b2ba6ed4cf9bb7464390e1f1430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d823fcb51992475ba5ebcdaa0bb6e75f",
            "placeholder": "​",
            "style": "IPY_MODEL_d237f66466e44e9883f93b75f26b5b71",
            "value": "model.safetensors: 100%"
          }
        },
        "dec9e68150a247d1b6a8e256c133dd38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_086dc71fba194729b435eb36a3b45ab6",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d80c549d93324f308e4a734134cba598",
            "value": 440449768
          }
        },
        "36540d305287438fb942310f668dd5cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec8c578e41d64ab7b4c18dde2bcfa721",
            "placeholder": "​",
            "style": "IPY_MODEL_02fd64b6d524484e945868dfcbf3a968",
            "value": " 440M/440M [00:04&lt;00:00, 110MB/s]"
          }
        },
        "809d0357c5b84268a49544b477b9b379": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d823fcb51992475ba5ebcdaa0bb6e75f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d237f66466e44e9883f93b75f26b5b71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "086dc71fba194729b435eb36a3b45ab6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d80c549d93324f308e4a734134cba598": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec8c578e41d64ab7b4c18dde2bcfa721": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02fd64b6d524484e945868dfcbf3a968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "\n",
        "\n",
        "\n",
        "*   What are LLMs ?\n",
        "*   LLM Pipeline\n",
        "*   Models and Tokenizers\n",
        "*   Batch inference\n",
        "*   Save and load models\n",
        "*   ModelHub\n",
        "\n"
      ],
      "metadata": {
        "id": "HzNmAbAe5pPC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Large Language Models\n",
        "\n",
        "Language models forms the foundation of Natural Languague Processing, is a machine learning model that once trained on a large set of data corpus, predicts the next most appropriate word, based on the context of the given text. LLMs are used for varied tasks including sentence classification, text Generation, language translation, question answering, text summarization\n",
        "\n",
        "<img src=\"https://github.com/argonne-lcf/llm-workshop/blob/main/tutorials/01-llm-101/images/transformer.png?raw=1\" width=\"400\">\n",
        "\n",
        "## Architecture\n",
        "Large Language Models uses the transformer architecture introduced by Vaswani et al. in the paper \"**Attention is All You Need**\".The transformer architecture has revolutionized NLP due to its parallelizability, scalability, and ability to capture long-range dependencies in text.\n",
        "\n",
        "Key components of the transformer architecture include:\n",
        "\n",
        "*   Self-Attention Mechanism: Allows the model to weigh the importance of different words in a sentence, enabling it to capture contextual information effectively.\n",
        "*   Positional Encoding: Injects information about the position of words in a sequence, helping the model understand word order.\n",
        "*   Feedforward Neural Networks: Process information from self-attention layers to generate output for each word/token.\n",
        "*   Layer Normalization and Residual Connections: Aid in stabilizing training and mitigating the vanishing gradient problem.\n",
        "*  Transformer Blocks: Comprised of multiple layers of self-attention and feedforward neural networks, stacked together to form the model.\n",
        "\n",
        "<img src=\"https://github.com/argonne-lcf/llm-workshop/blob/main/tutorials/01-llm-101/images/TransformerArch.png?raw=1\" width=\"400\">\n",
        "\n",
        "Source: [Attention Is all you need](https://arxiv.org/pdf/1706.03762.pdf)\n",
        "\n",
        "## Models\n",
        "There are a wide range of models available today, each with different model architectures, with varying number of model parameters trained on varied data corpus. The most well known among them are GPT3.5, GPT4, Bloom, Llamma7B, Llamma70B, and many more.\n",
        "\n",
        "<img src=\"https://github.com/argonne-lcf/llm-workshop/blob/main/tutorials/01-llm-101/images/models.png?raw=1\" width=\"400\">\n",
        "\n",
        "Source: [AILab](https://s10251.pcdn.co/wp-content/uploads/2024/02/2024-Alan-D-Thompson-AI-Bubbles-Planets-Rev-1.png)\n",
        "\n",
        "## What is Huggingface and the transformer library ?\n",
        "Several tools and libraries are available for working with Large Language Models. In this tutorial we will look at the **\"transformers\"** which is a popular library for natural language understanding and generation tasks, built on top of PyTorch and TensorFlow.\n",
        "\n",
        "<img src=\"https://github.com/argonne-lcf/llm-workshop/blob/main/tutorials/01-llm-101/images/HF.png?raw=1\" width=\"200\">\n",
        "\n",
        "Source: [HF](https://huggingface.co/)\n",
        "\n",
        "HuggingFace is a platform and community that provides open-source library tools and resources like pre-trained models and datasets.\n",
        "\n",
        "Refer to the following links for more information :\n",
        "\n",
        "*   https://huggingface.co/docs/hub/index\n",
        "*   https://huggingface.co/docs/transformers/en/index\n"
      ],
      "metadata": {
        "id": "45c16NIxWvlF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  "
      ],
      "metadata": {
        "id": "HnUbQEkN6Jwm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Pipeline\n",
        "\n",
        "Tasks, object creation, apply classifier and test the data.\n",
        "\n",
        "*   Preprocessing\n",
        "*   Applying model\n",
        "*   Post processing\n",
        "\n",
        "Several libraries, such as Hugging Face's \"transformers\" library, provide pre-built transformer pipelines that users can easily deploy and customize for their specific use cases. These pipelines abstract away the complexities of model integration and allow users to focus on their NLP tasks. Lets look into an example of such a pipeline.\n",
        "\n",
        "### Installation\n",
        "\n"
      ],
      "metadata": {
        "id": "XCKl2QjC7Fiy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXM__6PO5fCj",
        "outputId": "501d80c5-5d7b-4c48-b450-177952b7c1e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of pipeline for a classification task"
      ],
      "metadata": {
        "id": "q0iPMY4YHPj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "res = classifier(\"The panoramic view of the ocean was breathtaking\")\n",
        "print(res)\n",
        "\n",
        "res = classifier([\"The movie was boring and too long\", \"This restaurant is awesome\"])\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frbp6DDM7HxZ",
        "outputId": "18dcd4a3-83a6-460b-8267-ad4c4b9cca54"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9998416900634766}]\n",
            "[{'label': 'NEGATIVE', 'score': 0.9997920393943787}, {'label': 'POSITIVE', 'score': 0.9998743534088135}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of pipeline for a generation task\n",
        "\n",
        "This is an example of a simple pipeline for a text generation task. The pipeline can be instucted to use a specific model instead of using the default model('distilbert-base-uncased-finetuned-sst-2-english') by passing the \"model\" argument."
      ],
      "metadata": {
        "id": "dluOwQXt8l3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The goal of the Large Language model workshop is to \"\n",
        "\n",
        "generator = pipeline(\"text-generation\", model='gpt2')\n",
        "res = generator(prompt, max_length=25, num_return_sequences=5)\n",
        "\n",
        "for each in res:\n",
        "    print(each)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sR-bas198r5l",
        "outputId": "78627b5d-816a-4f43-89cb-09caec7c4131"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'generated_text': 'The goal of the Large Language model workshop is to \\xa0learn how each learner and teacher can implement multiple learning styles.'}\n",
            "{'generated_text': 'The goal of the Large Language model workshop is to \\xa0engage participants in setting up two informal conversations about this language and'}\n",
            "{'generated_text': 'The goal of the Large Language model workshop is to \\xa0promptly explore the possibility of developing an automatic language model for'}\n",
            "{'generated_text': 'The goal of the Large Language model workshop is to \\xa0develop a model of how we communicate in large numbers using the language'}\n",
            "{'generated_text': 'The goal of the Large Language model workshop is to \\xa0create an understanding of and understanding the role of language in the human'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refer to\n",
        "https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.pipeline.task that lists the different tasks that are supported as part of the pipleine.\n",
        "\n",
        "## Pipeline Components\n",
        "\n"
      ],
      "metadata": {
        "id": "ThBiMkgh99fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1 : Installations and imports\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# STEP 2 : Set up the prompt\n",
        "input_text = \"The panoramic view of the ocean was breathtaking.\"\n",
        "\n",
        "# STEP 3 : Load the pretrained model.\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "print(config)\n",
        "\n",
        "#STEP 4 : Load the tokenizer and tokenize the input text\n",
        "tokenizer  =  AutoTokenizer.from_pretrained(model_name)\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"]\n",
        "print(input_ids)\n",
        "\n",
        "# STEP 5 : Perform inference\n",
        "outputs = model(input_ids)\n",
        "result = outputs.logits\n",
        "print(result)\n",
        "\n",
        "# STEP 6 :  Interpret the output.\n",
        "probabilities = F.softmax(result, dim=-1)\n",
        "print(probabilities)\n",
        "predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
        "labels = [\"NEGATIVE\", \"POSITIVE\"]\n",
        "out_string = \"[{'label': '\" + str(labels[predicted_class]) + \"', 'score': \" + str(probabilities[0][predicted_class].tolist()) + \"}]\"\n",
        "print(out_string)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1SwXMzZ_Xq-",
        "outputId": "d15ef979-685f-411a-e30a-19c8af510b80"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"finetuning_task\": \"sst-2\",\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"NEGATIVE\",\n",
            "    \"1\": \"POSITIVE\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"NEGATIVE\": 0,\n",
            "    \"POSITIVE\": 1\n",
            "  },\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.35.2\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "tensor([[  101,  1996,  6090,  6525,  7712,  3193,  1997,  1996,  4153,  2001,\n",
            "          3052, 17904,  1012,   102]])\n",
            "tensor([[-4.2767,  4.5486]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[1.4695e-04, 9.9985e-01]], grad_fn=<SoftmaxBackward0>)\n",
            "[{'label': 'POSITIVE', 'score': 0.9998530149459839}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **How to set up a prompt?**\n",
        "\n",
        "A \"prompt\" refers to a specific input or query provided to a language model. They guide the text processing and generation by  providing the context for the model to generate coherent and relevant text based on the given input.\n",
        "\n",
        "The choice and structure of the prompt depends on the specific task, the context and desired output. Prompts can be \"discrete\" or \"instructive\" where they are explicit instructions or questions directed to the language model. They can also be more nuanced by more providing suggestions, directions and contexts to the model.  \n",
        "\n",
        "We will use very simple prompts in this tutorial section, but we will learn more about prompt engineering and how it helps in optimizing the performance of the model for a given use case in the following tutorials.\n"
      ],
      "metadata": {
        "id": "awmq0Kxojrt0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Pretrained Models**\n",
        "\n",
        "The AutoModelForSequenceClassification.from_pretrained() method instantiates a sequence classification model. Refer to https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodels for the list of model class.\n",
        "\n",
        "\"from_pretrained\" method downloads the pre-trained weights from the Hugging Face Model Hub or the specified URL if the model is not already cached locally. It then loads the weights into the instantiated model, initializing the model parameters with the pre-trained values.\n",
        "\n",
        "The model cache contains:\n",
        "*   model configuration (config.json)\n",
        "*   pretrained model weights (model.safetensors)\n",
        "*   tokenizer information (tokenizer.json, vocab.json, merges.txt, tokenizer.model)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZuhkpKyY4OfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Tokenization**\n",
        "\n",
        "Tokenization is a data preprocessing step which transforms the raw text data into a format suitable for machine learning models. Tokenizers break down raw text into smaller units called **tokens**. These tokens are what is fed into the language models. Based on the type and configuration of the tokenizer, these tokens can be words, subwords, or characters.\n",
        "\n",
        "**Types of tokenizers:**\n",
        "*   Character Tokenizers: Split text into individual characters.\n",
        "*   Word Tokenizers: Split text into words based on whitespace or punctuation.\n",
        "*   Subword Tokenizers: Split text into subword units, such as morphemes or character n-grams. Common subword tokenization algorithms include Byte-Pair Encoding (BPE), SentencePiece, and WordPiece.\n",
        "\n",
        "<img src=\"https://github.com/argonne-lcf/llm-workshop/blob/main/tutorials/01-llm-101/images/tokenization.png?raw=1\" width=\"600\">\n",
        "\n",
        "Source: [nlpiation](https://nlpiation.medium.com/how-to-use-huggingfaces-transformers-pre-trained-tokenizers-e029e8d6d1fa)\n",
        "\n",
        "\n",
        "**Vocabulary**: The \"vocabulary\" of a model refers to the set of words that the model has been trained to understand and use. Each of these words or subwords have a one-to-one numerical mapping.\n",
        "\n",
        "**Special Tokens**: Tokenizers may also include special tokens such as [CLS] (classification token), [SEP] (separator token), [UNK] (unknown token), and [PAD] (padding token). These tokens serve specific purposes in certain NLP tasks and help the model understand the structure of the input.\n",
        "\n",
        "**Tokenization Libraries:** Wellknown tokenization libraries include: Hugging Face Tokenizers, NLTK (Natural Language Toolkit), Spacy etc.\n",
        "\n",
        "**AutoTokenizer.from_pretrained** is a method provided by the HuggingFace Transformers library for loading a tokenizer from a pretrained model configuration and provides functionality or methods needed to preprocess text data for input to a pretrained model outputs the **input_ids** that represent the numerical representation of the raw text. Some of the models have additional information from tokenization like **attention mask** which is a binary mask indicating which tokens in an input sequence should be attended to by the model and which tokens should be ignored. return_tensors=\"pt\" indicates that the output will be of type pytorch tensors.\n",
        "\n",
        "Lets look at some the methods that are used for tokenization of the data."
      ],
      "metadata": {
        "id": "lMIIQZCcBbuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = \"I am working on a tutorial\"\n",
        "\n",
        "# get the vocabulary\n",
        "vocab = tokenizer.vocab\n",
        "# Number of entries to print\n",
        "n = 10\n",
        "\n",
        "# Print subset of the vocabulary\n",
        "print(\"Subset of tokenizer.vocab:\")\n",
        "for i, (token, index) in enumerate(tokenizer.vocab.items()):\n",
        "    print(f\"{token}: {index}\")\n",
        "    if i >= n - 1:\n",
        "        break\n",
        "\n",
        "print(\"Vocab size of the tokenizer = \", len(vocab))\n",
        "print(\"------------------------------------------\")\n",
        "\n",
        "# .tokenize chunks the existing sequence into different tokens based on the rules and vocab of the tokenizer.\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "print(\"Tokens : \", tokens)\n",
        "print(\"------------------------------------------\")\n",
        "\n",
        "# .convert_tokens_to_ids or .encode or .tokenize converts the tokens to their corresponding numerical representation.\n",
        "#  .convert_tokens_to_ids has a 1-1 mapping between tokens and numerical representation\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"encoded Ids: \", ids)\n",
        "\n",
        "# .encode also adds additional information like Start of sequence tokens and End of sequene\n",
        "print(\"tokenized sequence : \", tokenizer.encode(sequence))\n",
        "\n",
        "# .tokenizer has additional information about attention_mask.\n",
        "encode = tokenizer(sequence)\n",
        "print(\"Encode sequence : \", encode)\n",
        "print(\"------------------------------------------\")\n",
        "\n",
        "# .decode decodes the ids to raw text\n",
        "decode = tokenizer.decode(ids)\n",
        "print(\"Decode sequence : \", decode)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEttCIr4Bnnz",
        "outputId": "acde8bd1-4a87-4a3f-89f7-ee9b082ee3b5"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset of tokenizer.vocab:\n",
            "sri: 5185\n",
            "extravagant: 27856\n",
            "enthusiasm: 12024\n",
            "##ม: 29951\n",
            "ace: 9078\n",
            "qin: 19781\n",
            "surgeon: 9431\n",
            "disrepair: 27799\n",
            "##formed: 29021\n",
            "##！: 30512\n",
            "Vocab size of the tokenizer =  30522\n",
            "------------------------------------------\n",
            "Tokens :  ['i', 'am', 'working', 'on', 'a', 'tutor', '##ial']\n",
            "------------------------------------------\n",
            "encoded Ids:  [1045, 2572, 2551, 2006, 1037, 14924, 4818]\n",
            "tokenized sequence :  [101, 1045, 2572, 2551, 2006, 1037, 14924, 4818, 102]\n",
            "Encode sequence :  {'input_ids': [101, 1045, 2572, 2551, 2006, 1037, 14924, 4818, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "------------------------------------------\n",
            "Decode sequence :  i am working on a tutorial\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization with truncation\n",
        "sequence = \"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder.\"\n",
        "\n",
        "print(len(sequence))\n",
        "encoder = (tokenizer(sequence))\n",
        "print(len(encoder[\"input_ids\"]))\n",
        "encoder = (tokenizer(sequence, max_length=30,truncation=True))\n",
        "print(len(encoder[\"input_ids\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZSpSh4Vxduu",
        "outputId": "a8227f53-9a8a-4fa8-dc96-3be16bf0a329"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "144\n",
            "33\n",
            "30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Inference\n",
        "\n",
        "Here is another example of batched inference, where multiple samples are fed to the classifier pipeline and classifies each sentence as positive or negative. As noted, the inputs in the batch are of different lengths, so `padding`=`True` , helps in ensuring that all the inputs of same lengths by padding zeros at the end, but the `attention_mask` indicates which tokens to attend to and which ones to ignore.\n",
        "\n",
        "Note : `'max_length'` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hnLJhOR4DyI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer  =  AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "X_train = ['This is the first sample',\n",
        "           'This is the second sample but I am longest in the batch',\n",
        "           'This is the last sample but short']\n",
        "\n",
        "batch = tokenizer(X_train, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
        "print(batch)\n",
        "\n",
        "result = classifier(X_train)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I1_clpjD8cq",
        "outputId": "bffd5776-818f-4982-b684-581791ef4802"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  2023,  2003,  1996,  2034,  7099,   102,     0,     0,     0,\n",
            "             0,     0,     0,     0],\n",
            "        [  101,  2023,  2003,  1996,  2117,  7099,  2021,  1045,  2572,  6493,\n",
            "          1999,  1996, 14108,   102],\n",
            "        [  101,  2023,  2003,  1996,  2197,  7099,  2021,  2460,   102,     0,\n",
            "             0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])}\n",
            "[{'label': 'POSITIVE', 'score': 0.9916760921478271}, {'label': 'NEGATIVE', 'score': 0.9906133413314819}, {'label': 'NEGATIVE', 'score': 0.9917491674423218}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained model.\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "print(\"Batched inference with padding \")\n",
        "# Load the tokenizer and tokenize the input text with padding = True\n",
        "tokenizer  =  AutoTokenizer.from_pretrained(model_name)\n",
        "batch = tokenizer(X_train, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
        "input_ids = batch[\"input_ids\"]\n",
        "attention_mask = batch[\"attention_mask\"]\n",
        "print(batch)\n",
        "\n",
        "# Perform inference on padded inputs\n",
        "outputs = model(input_ids, attention_mask )\n",
        "result = outputs.logits\n",
        "print(result)\n",
        "\n",
        "print(\"________________________________________________________________\")\n",
        "\n",
        "print(\"Batched inference with Truncation \")\n",
        "\n",
        "batch = tokenizer(X_train, padding=False, truncation=True, max_length=5, return_tensors='pt')\n",
        "input_ids = batch[\"input_ids\"]\n",
        "attention_mask = batch[\"attention_mask\"]\n",
        "print(batch)\n",
        "outputs = model(input_ids, attention_mask )\n",
        "result = outputs.logits\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Da6TUzeBzvJX",
        "outputId": "31fa1b49-bc69-4dfe-dd09-590f4b602a8e"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batched inference with padding \n",
            "{'input_ids': tensor([[  101,  2023,  2003,  1996,  2034,  7099,   102,     0,     0,     0,\n",
            "             0,     0,     0,     0],\n",
            "        [  101,  2023,  2003,  1996,  2117,  7099,  2021,  1045,  2572,  6493,\n",
            "          1999,  1996, 14108,   102],\n",
            "        [  101,  2023,  2003,  1996,  2197,  7099,  2021,  2460,   102,     0,\n",
            "             0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])}\n",
            "tensor([[-2.4335,  2.3467],\n",
            "        [ 2.5392, -2.1198],\n",
            "        [ 2.5990, -2.1902]], grad_fn=<AddmmBackward0>)\n",
            "________________________________________________________________\n",
            "Batched inference with Truncation \n",
            "{'input_ids': tensor([[ 101, 2023, 2003, 1996,  102],\n",
            "        [ 101, 2023, 2003, 1996,  102],\n",
            "        [ 101, 2023, 2003, 1996,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1]])}\n",
            "tensor([[-3.1292,  3.2483],\n",
            "        [-3.1292,  3.2483],\n",
            "        [-3.1292,  3.2483]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save and Load models and tokenizer\n",
        "\n",
        "Model can be saved and loaded to and from a local model directory."
      ],
      "metadata": {
        "id": "_UgftafdFmUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "# Instantiate and train or fine-tune a model\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Train or fine-tune the model...\n",
        "\n",
        "# Save the model to a local directory\n",
        "directory = \"my_local_model\"\n",
        "model.save_pretrained(directory)\n",
        "\n",
        "# Load a pre-trained model from a local directory\n",
        "loaded_model = AutoModel.from_pretrained(directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "b106493d29c740298ee9bbd4ba636dcf",
            "f499c7cdd9bd48e29f40727bf1e4eab5",
            "304aa1944f1e41f5a091f3d170504e5c",
            "5fef9c01f74f463ca34d56ef580ae159",
            "4ac0c2b939de420cb778b815de33c7a2",
            "25c3b8cf32974fc68cb03db68bfb2fe1",
            "fe8638606bd842e3ad2753402552a398",
            "9b22799bd9314bec97a327e2397d73bb",
            "8d3d69a1ef384905b5b9dc9b991165e1",
            "fae993459e354a37af0dfa7226ed2b46",
            "d741e362e04b48c0945d75da046938ca",
            "c2b1ad9b2be94829a2d97fe60281e7a9",
            "0c374b2ba6ed4cf9bb7464390e1f1430",
            "dec9e68150a247d1b6a8e256c133dd38",
            "36540d305287438fb942310f668dd5cf",
            "809d0357c5b84268a49544b477b9b379",
            "d823fcb51992475ba5ebcdaa0bb6e75f",
            "d237f66466e44e9883f93b75f26b5b71",
            "086dc71fba194729b435eb36a3b45ab6",
            "d80c549d93324f308e4a734134cba598",
            "ec8c578e41d64ab7b4c18dde2bcfa721",
            "02fd64b6d524484e945868dfcbf3a968"
          ]
        },
        "id": "yDQ01NI04hXX",
        "outputId": "89a5c1a9-a4fc-493f-8867-04427f621e10"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b106493d29c740298ee9bbd4ba636dcf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2b1ad9b2be94829a2d97fe60281e7a9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Hub\n",
        "\n",
        "The Model Hub is\n",
        "*   where the members of the Hugging Face community can host all of their model checkpoints for simple storage, discovery, and sharing.\n",
        "*   Download pre-trained models with the huggingface_hub client library, with Transformers for fine-tuning\n",
        "* Make use of Inference API to use models in production settings.\n",
        "\n",
        "https://huggingface.co/docs/hub/en/models-the-hub\n",
        "\n"
      ],
      "metadata": {
        "id": "UOyKcWqDGeFA"
      }
    }
  ]
}